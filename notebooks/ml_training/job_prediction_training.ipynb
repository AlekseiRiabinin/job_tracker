{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da44f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# job_prediction_training.ipynb\n",
    "# Jupyter Notebook for Job Application Success Prediction Model Training\n",
    "\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import joblib\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Self, Pattern\n",
    "from datetime import datetime\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score,\n",
    "    explained_variance_score\n",
    ")\n",
    "from langdetect import detect\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as EN_STOP_WORDS\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as DE_STOP_WORDS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18add365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "\n",
    "MODEL_VERSION = \"1.0\"\n",
    "\n",
    "TECH_KEYWORDS = {\n",
    "    'python': r'\\bpython\\b',\n",
    "    'scala': r'\\bscala\\b',\n",
    "    'spark': r'\\bspark\\b|\\bpyspark\\b',\n",
    "    'kafka': r'\\bkafka\\b',\n",
    "    'airflow': r'\\bairflow\\b',\n",
    "    'mongodb': r'\\bmongo(db)?\\b',\n",
    "    'flask': r'\\bflask\\b',\n",
    "    'sql': r'\\bsql\\b|\\bpostgresql\\b|\\bpl/pgsql\\b',\n",
    "    'docker': r'\\bdocker\\b',\n",
    "    'tensorflow': r'\\btensorflow\\b',\n",
    "    'fastapi': r'\\bfastapi\\b'\n",
    "}\n",
    "\n",
    "INDUSTRY_PATTERNS = {\n",
    "    'en': {\n",
    "        'finance': (\n",
    "            r'\\b(financ|bank|invest|accounting|'\n",
    "            r'audit|tax|trad|republic|brokerage)\\w*\\b'\n",
    "        ),\n",
    "        'healthcare': (\n",
    "            r'\\b(health|medical|pharma|'\n",
    "            r'hospital|clinic|patient)\\w*\\b'\n",
    "        ),\n",
    "        'tech': (\n",
    "            r'\\b(tech|software|it|computer|system|'\n",
    "            r'developer|engineer|ai|artificial intelligence|'\n",
    "            r'deep learning|machine learning|cloud|data science)\\w*\\b'\n",
    "        ),\n",
    "        'manufacturing': (\n",
    "            r'\\b(manufactur|production|factory|'\n",
    "            r'plant|assembly|automotive|car|vehicle)\\w*\\b'\n",
    "        ),\n",
    "        'retail': (\n",
    "            r'\\b(retail|store|shop|'\n",
    "            r'e.?commerce|merchandis)\\w*\\b'\n",
    "        ),\n",
    "        'logistics': (\n",
    "            r'\\b(logistics|delivery|'\n",
    "            r'supply chain|transport|shipping)\\w*\\b'\n",
    "        ),\n",
    "        'food': r'\\b(food|grocery|restaurant|meal)\\w*\\b',\n",
    "        'telecom': r'\\b(telecom|telecommunication|mobile network)\\w*\\b',\n",
    "        'energy': r'\\b(energy|solar|renewable|power|electric)\\w*\\b',\n",
    "        'aerospace': r'\\b(aerospace|aviation|space|satellite)\\w*\\b',\n",
    "        'consulting': r'\\b(consult|advisory)\\w*\\b',\n",
    "        'insurance': r'\\b(insurance|actuar)\\w*\\b',\n",
    "        'gaming': r'\\b(game|gaming)\\w*\\b',\n",
    "        'social': r'\\b(social media|social network)\\w*\\b',\n",
    "        'semiconductor': r'\\b(semiconductor|chip|microelectronic)\\w*\\b'\n",
    "    },\n",
    "    'de': {\n",
    "        'finance': (\n",
    "            r'\\b(finan|bank|invest|buchhalt|'\n",
    "            r'rechnungs|steuer|handel)\\w*\\b'\n",
    "        ),\n",
    "        'healthcare': (\n",
    "            r'\\b(gesundheit|medizin|pharma|'\n",
    "            r'krankenhaus|klinik|patient)\\w*\\b'\n",
    "        ),\n",
    "        'tech': (\n",
    "            r'\\b(tech|software|it|computer|system|'\n",
    "            r'entwickler|ingenieur|ki|kÃ¼nstliche intelligenz|'\n",
    "            r'maschinelles lernen|datenwissenschaft)\\w*\\b'\n",
    "        ),\n",
    "        'manufacturing': (\n",
    "            r'\\b(produktion|fabrik|werk|'\n",
    "            r'montage|herstellung|auto|fahrzeug)\\w*\\b'\n",
    "        ),\n",
    "        'retail': (\n",
    "            r'\\b(einzelhandel|laden|'\n",
    "            r'geschÃ¤ft|e.?commerce|handel)\\w*\\b'\n",
    "        ),\n",
    "        'logistics': r'\\b(logistik|lieferung|transport)\\w*\\b',\n",
    "        'food': r'\\b(lebensmittel|nahrung|restaurant|mahlzeit)\\w*\\b',\n",
    "        'telecom': r'\\b(telekommunikation|mobilfunk)\\w*\\b',\n",
    "        'energy': r'\\b(energie|solar|erneuerbar|strom)\\w*\\b',\n",
    "        'aerospace': r'\\b(luftfahrt|raumfahrt|satellit)\\w*\\b',\n",
    "        'consulting': r'\\b(beratung|berater)\\w*\\b',\n",
    "        'insurance': r'\\b(versicherung)\\w*\\b',\n",
    "        'gaming': r'\\b(spiel|gaming)\\w*\\b',\n",
    "        'social': r'\\b(soziale medien)\\w*\\b',\n",
    "        'semiconductor': r'\\b(hÃ¤lbleiter|chip|mikroelektronik)\\w*\\b'\n",
    "    }\n",
    "}\n",
    "\n",
    "LANGUAGE_PATTERNS: dict[str, Pattern] = {\n",
    "    'german': re.compile(\n",
    "        r'(?:flieÃŸend|verhandlungssicher|gut|geschÃ¤ftssicher|erforderlich|voraussetzung|kenntnisse|sprachkenntnisse)\\s+deutsch|'\n",
    "        r'deutsch\\s*(?:kenntnisse|erforderlich|voraussetzung|sprachkenntnisse|erwÃ¼nscht|muss|sollte)|'\n",
    "        r'german\\s+(?:required|necessary|fluent|proficient|knowledge|skills|language)|'\n",
    "        r'deutschkenntnisse|'\n",
    "        r'german\\s+language',\n",
    "        re.IGNORECASE\n",
    "    ),\n",
    "    \n",
    "    'english': re.compile(\n",
    "        r'english\\s+(?:fluent|proficient|required|necessary|working\\s+knowledge|skills)|'\n",
    "        r'englisch\\s+(?:flieÃŸend|verhandlungssicher|erforderlich|voraussetzung|kenntnisse)|'\n",
    "        r'business\\s+english|'\n",
    "        r'englischkenntnisse',\n",
    "        re.IGNORECASE\n",
    "    ),\n",
    "    \n",
    "    'german_levels': {\n",
    "        'A1': re.compile(r'A1|A1\\s*deutsch|grundkenntnisse|anfÃ¤ngerkenntnisse|basic\\s+german', re.IGNORECASE),\n",
    "        'A2': re.compile(r'A2|A2\\s*deutsch|basiskenntnisse|einfache\\s+konversation|elementary', re.IGNORECASE),\n",
    "        'B1': re.compile(r'B1|B1\\s*deutsch|fortgeschrittene\\s+kenntnisse|selbstÃ¤ndige\\s+sprachverwendung|intermediate', re.IGNORECASE),\n",
    "        'B2': re.compile(r'B2|B2\\s*deutsch|gute\\s+kenntnisse|berufsbezogene\\s+sprachkenntnisse|good|confident', re.IGNORECASE),  # Added simple 'B2'\n",
    "        'C1': re.compile(r'C1|C1\\s*deutsch|verhandlungssicher|flieÃŸend|geschÃ¤ftssicher|advanced|fluent|business', re.IGNORECASE),\n",
    "        'C2': re.compile(r'C2|C2\\s*deutsch|muttersprachlich|herausragende\\s+kenntnisse|native|proficient', re.IGNORECASE)\n",
    "    }\n",
    "}\n",
    "\n",
    "SENIORITY_PATTERNS = {\n",
    "    'senior': re.compile(r'\\bsenior\\b', re.IGNORECASE),\n",
    "    'junior': re.compile(r'\\bjunior\\b', re.IGNORECASE)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee14f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file path: /home/aleksei/Projects/job_tracker/data/samples/job_applications.json\n",
      "Model directory: /home/aleksei/Projects/job_tracker/app/services/job_predictor/models\n",
      "Spacy directory: /home/aleksei/Projects/job_tracker/app/services/job_predictor/models/spacy\n"
     ]
    }
   ],
   "source": [
    "# Configuration for local data\n",
    "class Config:\n",
    "    BASE_DIR = Path.cwd().parent.parent\n",
    "    \n",
    "    DATA_FILE = BASE_DIR / \"data\" / \"samples\" / \"job_applications.json\"\n",
    "    MODEL_DIR = BASE_DIR / \"app\" / \"services\" / \"job_predictor\" / \"models\"\n",
    "    SPACY_MODEL_DIR = BASE_DIR / \"app\" / \"services\" / \"job_predictor\" / \"models\" / \"spacy\"\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(config.DATA_FILE.parent, exist_ok=True)\n",
    "os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(config.SPACY_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data file path: {config.DATA_FILE}\")\n",
    "print(f\"Model directory: {config.MODEL_DIR}\")\n",
    "print(f\"Spacy directory: {config.SPACY_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3510b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  de_core_news_sm not found. Please download it:\n",
      "   python -m spacy download de_core_news_sm --direct --model-path /home/aleksei/Projects/job_tracker/app/services/job_predictor/models/spacy\n",
      "âš ï¸  en_core_web_sm not found. Please download it:\n",
      "   python -m spacy download en_core_web_sm --direct --model-path /home/aleksei/Projects/job_tracker/app/services/job_predictor/models/spacy\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy models with custom paths\n",
    "def load_spacy_model(\n",
    "        model_name: str,\n",
    "        custom_path: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"Load spaCy model from custom path or default location.\"\"\"\n",
    "    try:\n",
    "        if custom_path and os.path.exists(custom_path):\n",
    "            # Try custom path first\n",
    "            model_path = os.path.join(custom_path, model_name)\n",
    "            if os.path.exists(model_path):\n",
    "                return spacy.load(model_path)\n",
    "\n",
    "        # Fallback to default loading\n",
    "        return spacy.load(model_name)\n",
    "        \n",
    "    except OSError:\n",
    "        print(f\"âš ï¸  {model_name} not found. Please download it:\")\n",
    "        print(f\"   python -m spacy download {model_name} --direct --model-path {custom_path}\")\n",
    "        return None\n",
    "\n",
    "# Load models with custom paths\n",
    "nlp_de = load_spacy_model(\"de_core_news_sm\", config.SPACY_MODEL_DIR)\n",
    "nlp_en = load_spacy_model(\"en_core_web_sm\", config.SPACY_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49e43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced text preprocessing with fallback\n",
    "def preprocess_text(text: str, lang: str) -> str:\n",
    "    \"\"\"Language-specific preprocessing with robust error handling.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        if lang == 'de' and nlp_de is not None:\n",
    "            doc = nlp_de(text)\n",
    "        elif nlp_en is not None:\n",
    "            doc = nlp_en(text)\n",
    "        else:\n",
    "            # Fallback: simple tokenization without spaCy\n",
    "            return simple_tokenize(text)\n",
    "        \n",
    "        return \" \".join([\n",
    "            token.lemma_.lower() for token in doc \n",
    "            if not token.is_stop and token.is_alpha\n",
    "        ])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  spaCy processing failed: {e}\")\n",
    "        return simple_tokenize(text)\n",
    "\n",
    "def simple_tokenize(text: str) -> str:\n",
    "    \"\"\"Fallback tokenization without spaCy.\"\"\"\n",
    "    # Basic tokenization and cleaning\n",
    "    tokens = re.findall(r'\\b[a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ]+\\b', text.lower())\n",
    "    # Remove stopwords (basic lists)\n",
    "    en_stopwords = {'the', 'and', 'is', 'in', 'to', 'of', 'for', 'with', 'on', 'at'}\n",
    "    de_stopwords = {'der', 'die', 'das', 'und', 'in', 'zu', 'den', 'von', 'mit', 'fÃ¼r'}\n",
    "    stopwords = en_stopwords.union(de_stopwords)\n",
    "    \n",
    "    return \" \".join([token for token in tokens if token not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a252d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loaded 1000 records from /home/aleksei/Projects/job_tracker/data/samples/job_applications.json\n"
     ]
    }
   ],
   "source": [
    "# Data Loading from JSON file\n",
    "def load_training_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load training data that matches the application data model.\"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert date strings to datetime objects\n",
    "        date_columns = ['applied_date', 'response_date', 'last_prediction_date']\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "        \n",
    "        # Extract ml_meta fields (matches data model structure)\n",
    "        if 'ml_meta' in df.columns:\n",
    "            ml_fields = ['success_probability', 'german_level', 'last_prediction_date']\n",
    "            for field in ml_fields:\n",
    "                df[field] = df['ml_meta'].apply(\n",
    "                    lambda x: x.get(field) if isinstance(x, dict) else None\n",
    "                )\n",
    "\n",
    "        if 'last_prediction_date' in df.columns:\n",
    "            df['last_prediction_date'] = pd.to_datetime(df['last_prediction_date'])\n",
    "\n",
    "        print(f\"ğŸ“Š Loaded {len(df)} records from {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ {e}\")\n",
    "        print(\"ğŸ’¡ Please run data_generator.py first to create sample data\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load data\n",
    "df = load_training_data(config.DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c2de1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Data Exploration\n",
      "Dataset shape: (1000, 16)\n",
      "\n",
      "ğŸ“‹ Data Types:\n",
      "company                         object\n",
      "location                        object\n",
      "role                            object\n",
      "status                          object\n",
      "source                          object\n",
      "applied_date            datetime64[ns]\n",
      "response_date           datetime64[ns]\n",
      "response_days                  float64\n",
      "notes                           object\n",
      "vacancy_description             object\n",
      "description_lang                object\n",
      "ml_meta                         object\n",
      "requirements                    object\n",
      "success_probability            float64\n",
      "german_level                    object\n",
      "last_prediction_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "ğŸ” Missing Values:\n",
      "company                   0\n",
      "location                  0\n",
      "role                      0\n",
      "status                    0\n",
      "source                    0\n",
      "applied_date              0\n",
      "response_date           297\n",
      "response_days           297\n",
      "notes                   259\n",
      "vacancy_description       0\n",
      "description_lang          0\n",
      "ml_meta                   0\n",
      "requirements              0\n",
      "success_probability       0\n",
      "german_level            143\n",
      "last_prediction_date      0\n",
      "dtype: int64\n",
      "\n",
      "ğŸ¯ Target Distribution:\n",
      "count    1000.000000\n",
      "mean        0.500950\n",
      "std         0.288633\n",
      "min         0.000000\n",
      "25%         0.250000\n",
      "50%         0.510000\n",
      "75%         0.740000\n",
      "max         1.000000\n",
      "Name: success_probability, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration\n",
    "if not df.empty:\n",
    "    print(\"ğŸ“ˆ Data Exploration\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nğŸ“‹ Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nğŸ” Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    if 'success_probability' in df.columns:\n",
    "        print(\"\\nğŸ¯ Target Distribution:\")\n",
    "        print(df['success_probability'].describe())\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # df['success_probability'].hist(bins=20)\n",
    "        # plt.title('Distribution of Success Probability')\n",
    "        # plt.xlabel('Success Probability')\n",
    "        # plt.ylabel('Frequency')\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68fa6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (Updated for local data)\n",
    "def create_enhanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Feature engineering pipeline matching production code.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['description_lang'] = df['vacancy_description'].apply(\n",
    "        lambda x: (\n",
    "            detect(x[:500]) \n",
    "            if isinstance(x, str) and len(x) > 0 \n",
    "            else 'en')\n",
    "    )\n",
    "\n",
    "    def extract_tech_stack(description: str) -> dict:\n",
    "        if not isinstance(description, str):\n",
    "            return {}\n",
    "        return {\n",
    "            tech: int(bool(re.search(pattern, description.lower())))\n",
    "            for tech, pattern in TECH_KEYWORDS.items()\n",
    "        }\n",
    "    \n",
    "    df['tech_stack'] = df['vacancy_description'].apply(extract_tech_stack)\n",
    "    \n",
    "    def extract_language_requirements(description: str) -> dict:\n",
    "        if not isinstance(description, str):\n",
    "            return {\n",
    "                'german_required': False,\n",
    "                'english_required': False,\n",
    "                'german_level': None\n",
    "            }\n",
    "        \n",
    "        requirements = {\n",
    "            'german_required': (\n",
    "                bool(LANGUAGE_PATTERNS['german'].search(description))\n",
    "            ),\n",
    "            'english_required': (\n",
    "                bool(LANGUAGE_PATTERNS['english'].search(description))\n",
    "            ),\n",
    "            'german_level': None\n",
    "        }\n",
    "\n",
    "        if requirements['german_required']:\n",
    "            german_levels: dict[str, Pattern] = (\n",
    "                LANGUAGE_PATTERNS['german_levels']\n",
    "            )\n",
    "            for level, pattern in german_levels.items():\n",
    "                if pattern.search(description):\n",
    "                    requirements['german_level'] = level\n",
    "                    break\n",
    "\n",
    "        return requirements\n",
    "    \n",
    "    df['language_reqs'] = (\n",
    "        df['vacancy_description'].apply(extract_language_requirements)\n",
    "    )    \n",
    "\n",
    "    def extract_industry_focus(description: str, lang: str = 'en') -> str:\n",
    "        if not isinstance(description, str):\n",
    "            return 'other'\n",
    "        \n",
    "        description_lower = description.lower()\n",
    "        patterns = INDUSTRY_PATTERNS.get(lang, INDUSTRY_PATTERNS['en'])\n",
    "\n",
    "        for industry, pattern in patterns.items():\n",
    "            if re.search(pattern, description_lower):\n",
    "                return industry\n",
    "        return 'other'\n",
    "\n",
    "    df['industry'] = df.apply(\n",
    "        lambda x: extract_industry_focus(\n",
    "            x['vacancy_description'], x['description_lang']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    def calculate_seniority(role):\n",
    "        if not isinstance(role, str):\n",
    "            return 0\n",
    "        role_lower = role.lower()\n",
    "        if SENIORITY_PATTERNS['senior'].search(role_lower):\n",
    "            return 1\n",
    "        elif SENIORITY_PATTERNS['junior'].search(role_lower):\n",
    "            return -1\n",
    "        return 0\n",
    "    \n",
    "    df['relative_seniority'] = df['role'].apply(calculate_seniority)\n",
    "    \n",
    "    def preprocess_text(text: str, lang: str) -> str:\n",
    "        \"\"\"Language-specific preprocessing with error handling.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            if lang == 'de' and nlp_de is not None:\n",
    "                doc = nlp_de(text)\n",
    "            elif nlp_en is not None:\n",
    "                doc = nlp_en(text)\n",
    "            else:\n",
    "                return simple_tokenize(text)\n",
    "            \n",
    "            return \" \".join([\n",
    "                token.lemma_.lower() for token in doc \n",
    "                if not token.is_stop and token.is_alpha\n",
    "            ])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"spaCy processing failed: {e}\")\n",
    "            return simple_tokenize(text)\n",
    "\n",
    "    def simple_tokenize(text: str) -> str:\n",
    "        \"\"\"Fallback tokenization without spaCy.\"\"\"\n",
    "        tokens = re.findall(r'\\b[a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ]+\\b', text.lower())\n",
    "\n",
    "        en_stopwords = {\n",
    "            'the', 'and', 'is', 'in', 'to', 'of', 'for', 'with', 'on', 'at'\n",
    "        }\n",
    "        de_stopwords = {\n",
    "            'der', 'die', 'das', 'und', 'in', 'zu', 'den', 'von', 'mit', 'fÃ¼r'\n",
    "        }\n",
    "        stopwords = en_stopwords.union(de_stopwords)\n",
    "        \n",
    "        return \" \".join([token for token in tokens if token not in stopwords])\n",
    "\n",
    "    df['processed_text'] = df.apply(\n",
    "        lambda x: preprocess_text(\n",
    "            x['vacancy_description'], x['description_lang']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    lang_reqs = pd.json_normalize(df['language_reqs'])\n",
    "    df = pd.concat([df, lang_reqs.add_prefix('lang_')], axis=1)\n",
    "\n",
    "    df.drop(columns=['language_reqs'], inplace=True, errors='ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796947bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Production feature engineering completed\n",
      "Enhanced dataset shape: (1000, 23)\n",
      "Available features: ['company', 'location', 'role', 'status', 'source', 'applied_date', 'response_date', 'response_days', 'notes', 'vacancy_description', 'description_lang', 'ml_meta', 'requirements', 'success_probability', 'german_level', 'last_prediction_date', 'tech_stack', 'industry', 'relative_seniority', 'processed_text', 'lang_german_required', 'lang_english_required', 'lang_german_level']\n"
     ]
    }
   ],
   "source": [
    "# Apply Production Feature Engineering\n",
    "if not df.empty:\n",
    "    enhanced_df = create_enhanced_features(df)\n",
    "    print(\"âœ… Production feature engineering completed\")\n",
    "    print(f\"Enhanced dataset shape: {enhanced_df.shape}\")\n",
    "    print(f\"Available features: {list(enhanced_df.columns)}\")\n",
    "    enhanced_df.head()\n",
    "else:\n",
    "    enhanced_df = pd.DataFrame()\n",
    "    print(\"âš ï¸  No data available for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500a2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Final training set: 1000 samples\n",
      "ğŸ¯ Target distribution: 0.50095 mean\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "def prepare_training_data(\n",
    "        enhanced_df: pd.DataFrame,\n",
    "        target_column='success_probability'\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Prepare data for training from enhanced features.\"\"\"\n",
    "    if enhanced_df.empty:\n",
    "        return pd.DataFrame(), pd.Series(), pd.Series()\n",
    "    \n",
    "    # Filter out missing target values\n",
    "    df_clean = enhanced_df.dropna(subset=[target_column])\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        print(\"âš ï¸  No data with target values available\")\n",
    "        return pd.DataFrame(), pd.Series(), pd.Series()\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df_clean\n",
    "    y = df_clean[target_column] \n",
    " \n",
    "    print(f\"ğŸ“Š Final training set: {X.shape[0]} samples\")\n",
    "    print(f\"ğŸ¯ Target distribution: {pd.Series(y).mean()} mean\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "if not enhanced_df.empty:\n",
    "    X, y = prepare_training_data(enhanced_df)\n",
    "else:\n",
    "    X, y = pd.DataFrame(), pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TechStackTransformer Class\n",
    "class TechStackTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms a column of tech stack dicts into a feature matrix.\"\"\"\n",
    "    \n",
    "    def __init__(self: Self) -> None:\n",
    "        \"\"\"Initialize the transformer.\"\"\"\n",
    "        self.feature_names_ = None\n",
    "        self.tech_keywords_ = list(TECH_KEYWORDS.keys())\n",
    "    \n",
    "    def fit(self: Self, X: pd.DataFrame, y: pd.Series = None) -> Self:\n",
    "        \"\"\"Learn the feature names from the tech stack dictionaries.\"\"\"\n",
    "        self.feature_names_ = sorted(self.tech_keywords_)\n",
    "        return self\n",
    "    \n",
    "    def transform(self: Self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform tech stack dicts into a binary feature matrix.\"\"\"\n",
    "        if self.feature_names_ is None:\n",
    "            raise ValueError(\"Must call fit() before transform()\")\n",
    "        \n",
    "        # Handle both Series and DataFrame input\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            tech_series = X['tech_stack']\n",
    "        else:\n",
    "            tech_series = X\n",
    "        \n",
    "        # Create a DataFrame with 0/1 values for each tech\n",
    "        transformed_data = []\n",
    "        for tech_dict in tech_series:\n",
    "            row = {tech: 0 for tech in self.feature_names_}\n",
    "            if isinstance(tech_dict, dict):\n",
    "                for tech, value in tech_dict.items():\n",
    "                    if tech in row and value:\n",
    "                        row[tech] = 1\n",
    "            transformed_data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(transformed_data, columns=self.feature_names_)\n",
    "\n",
    "    def get_feature_names_out(self: Self) -> list[str]:\n",
    "        \"\"\"Get output feature names for transformation.\"\"\"\n",
    "        if self.feature_names_ is None:\n",
    "            raise ValueError(\"Transformer not fitted yet\")\n",
    "        return self.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73236e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAwareTfidf(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Language-aware TF-IDF vectorizer.\"\"\"\n",
    "    \n",
    "    def __init__(self: Self, max_features: int = 50) -> None:\n",
    "        \"\"\"Initialize the language-aware vectorizer.\"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer_en = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words=list(EN_STOP_WORDS)\n",
    "        )\n",
    "        self.vectorizer_de = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words=list(DE_STOP_WORDS)\n",
    "        )\n",
    "        self.feature_names_ = None\n",
    "        self.has_en_data = False\n",
    "        self.has_de_data = False\n",
    "\n",
    "    def fit(self: Self, X: pd.DataFrame, y=None) -> Self:\n",
    "        \"\"\"Fit the vectorizers to English and German text subsets.\"\"\"\n",
    "        if not {'processed_text', 'description_lang'}.issubset(X.columns):\n",
    "            raise ValueError(\n",
    "                f\"Input DataFrame must contain 'processed_text' \"\n",
    "                f\"and 'description_lang' columns\"\n",
    "            )\n",
    "        \n",
    "        texts_en = X[X['description_lang'] == 'en']['processed_text']\n",
    "        texts_de = X[X['description_lang'] == 'de']['processed_text']\n",
    "\n",
    "        self.has_en_data = not texts_en.empty\n",
    "        self.has_de_data = not texts_de.empty\n",
    "\n",
    "        if self.has_en_data:\n",
    "            self.vectorizer_en.fit(texts_en)\n",
    "        if self.has_de_data:\n",
    "            self.vectorizer_de.fit(texts_de)\n",
    "\n",
    "        # Build feature names\n",
    "        en_features = [\n",
    "            f\"en_{f}\" for f in self.vectorizer_en.get_feature_names_out()\n",
    "        ] if self.has_en_data else []\n",
    "        \n",
    "        de_features = [\n",
    "            f\"de_{f}\" for f in self.vectorizer_de.get_feature_names_out()\n",
    "        ] if self.has_de_data else []\n",
    " \n",
    "        self.feature_names_ = en_features + de_features\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self: Self, X: pd.DataFrame) -> csr_matrix:\n",
    "        \"\"\"Transform text data into a combined TF-IDF feature matrix.\"\"\"\n",
    "        results = []\n",
    "        for _, row in X.iterrows():\n",
    "            text = row['processed_text']\n",
    "            lang = row['description_lang']\n",
    "            \n",
    "            if lang == 'en' and self.has_en_data:\n",
    "                results.append(self.vectorizer_en.transform([text]))\n",
    "            elif lang == 'de' and self.has_de_data:\n",
    "                results.append(self.vectorizer_de.transform([text]))\n",
    "            else:\n",
    "                if self.has_en_data:\n",
    "                    empty_vec = csr_matrix(\n",
    "                        (1, len(self.vectorizer_en.get_feature_names_out()))\n",
    "                    )\n",
    "                elif self.has_de_data:\n",
    "                    empty_vec = csr_matrix(\n",
    "                        (1, len(self.vectorizer_de.get_feature_names_out()))\n",
    "                    )\n",
    "                else:\n",
    "                    empty_vec = csr_matrix((1, 0))\n",
    "                results.append(empty_vec)\n",
    "\n",
    "        if results:\n",
    "            return vstack(results)\n",
    "        return csr_matrix((len(X), 0))\n",
    "    \n",
    "    def get_feature_names_out(self: Self) -> list[str]:\n",
    "        \"\"\"Get output feature names with language prefixes.\"\"\"\n",
    "        if self.feature_names_ is None:\n",
    "            raise ValueError(\"Transformer not fitted yet\")\n",
    "        return self.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "141af7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline\n",
    "def create_pipeline() -> Pipeline:\n",
    "    \"\"\"Create the model pipeline matching production.\"\"\"\n",
    "    preprocessor = make_column_transformer(\n",
    "        (TechStackTransformer(), ['tech_stack']),\n",
    "        (OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['industry']),\n",
    "        (OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['source']),\n",
    "        (OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['lang_german_level']),\n",
    "        ('passthrough', ['relative_seniority', 'lang_german_required', 'lang_english_required']),\n",
    "        (LanguageAwareTfidf(max_features=50), ['processed_text', 'description_lang']),\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    return make_pipeline(\n",
    "        preprocessor,\n",
    "        GradientBoostingRegressor(\n",
    "            n_estimators=150,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            random_state=config.RANDOM_STATE,\n",
    "            verbose=1,\n",
    "            loss='squared_error'\n",
    "        )\n",
    "    )\n",
    "\n",
    "pipeline = create_pipeline()\n",
    "print(\"âœ… Pipeline created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eabddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Train set: 800 samples\n",
      "ğŸ“Š Test set: 200 samples\n",
      "ğŸ¯ Target range - Train: [0.000, 1.000]\n",
      "ğŸ¯ Target range - Test: [0.010, 0.990]\n",
      "ğŸ“ˆ Target mean - Train: 0.494, Test: 0.531\n",
      "ğŸ”§ Features available: ['company', 'location', 'role', 'status', 'source', 'applied_date', 'response_date', 'response_days', 'notes', 'vacancy_description', 'description_lang', 'ml_meta', 'requirements', 'german_level', 'last_prediction_date', 'tech_stack', 'industry', 'relative_seniority', 'processed_text', 'lang_german_required', 'lang_english_required', 'lang_german_level']\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split\n",
    "if not enhanced_df.empty and 'success_probability' in enhanced_df.columns:\n",
    "    y = enhanced_df['success_probability']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        enhanced_df.drop(columns=['success_probability']),\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        stratify=None\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“Š Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"ğŸ“Š Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"ğŸ¯ Target range - Train: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "    print(f\"ğŸ¯ Target range - Test: [{y_test.min():.3f}, {y_test.max():.3f}]\")\n",
    "    print(f\"ğŸ“ˆ Target mean - Train: {y_train.mean():.3f}, Test: {y_test.mean():.3f}\")\n",
    "    \n",
    "    # Show available features for debugging (exclude target)\n",
    "    feature_columns = [col for col in enhanced_df.columns if col != 'success_probability']\n",
    "    print(f\"ğŸ”§ Features available: {feature_columns}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No data available for training\")\n",
    "    X_train, X_test, y_train, y_test = pd.DataFrame(), pd.DataFrame(), pd.Series(), pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d04ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Language distribution in training data:\n",
      "description_lang\n",
      "en    550\n",
      "de    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ” Sample of German texts (if any):\n",
      "Found 250 German texts\n",
      "Sample: zur verstÃ¤rkung unseres teams suchen wir einen senior data engineer jahre erfahrung optimierung perf...\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Language distribution in training data:\")\n",
    "print(X_train['description_lang'].value_counts())\n",
    "\n",
    "print(\"\\nğŸ” Sample of German texts (if any):\")\n",
    "german_texts = X_train[X_train['description_lang'] == 'de']['processed_text']\n",
    "if not german_texts.empty:\n",
    "    print(f\"Found {len(german_texts)} German texts\")\n",
    "    print(f\"Sample: {german_texts.iloc[0][:100]}...\")\n",
    "else:\n",
    "    print(\"No German texts found in training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking transformer signatures...\n",
      "âœ… columntransformer: ColumnTransformer has correct fit signature\n",
      "âœ… gradientboostingregressor: GradientBoostingRegressor has correct fit signature\n"
     ]
    }
   ],
   "source": [
    "# Quick debugging script\n",
    "def check_transformer_signatures(pipeline: Pipeline) -> None:\n",
    "    \"\"\"Check if all transformers have correct fit method signatures.\"\"\"\n",
    "    for step_name, transformer in pipeline.steps:\n",
    "        if hasattr(transformer, 'fit'):\n",
    "            # Check if fit method accepts y parameter\n",
    "            sig = inspect.signature(transformer.fit)\n",
    "            params = list(sig.parameters.keys())\n",
    "\n",
    "            if len(params) < 2 or 'y' not in params[1:]:\n",
    "                print(f\"âŒ {step_name}: {type(transformer).__name__} has incorrect fit signature\")\n",
    "                print(f\"   Expected: fit(self, X, y=None)\")\n",
    "                print(f\"   Actual: {str(sig)}\")\n",
    "            else:\n",
    "                print(f\"âœ… {step_name}: {type(transformer).__name__} has correct fit signature\")\n",
    "\n",
    "print(\"ğŸ” Checking transformer signatures...\")\n",
    "check_transformer_signatures(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4be126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ MODEL TRAINING WITH PRODUCTION FEATURES\n",
      "============================================================\n",
      "ğŸ“Š Starting training...\n",
      "   Training samples: 800\n",
      "   Features available: 22\n",
      "   Target distribution: {0.65: 16, 0.18: 16, 0.3: 15, 0.31: 13, 0.13: 13, 0.88: 13, 0.51: 12, 0.32: 12, 0.56: 11, 0.7: 11, 0.81: 11, 0.02: 11, 0.5: 11, 0.95: 11, 0.46: 11, 0.58: 10, 0.83: 10, 0.17: 10, 0.25: 10, 0.09: 10, 0.54: 10, 0.12: 10, 0.44: 10, 0.93: 10, 0.99: 9, 0.01: 9, 0.77: 9, 0.38: 9, 0.6: 9, 0.42: 9, 0.11: 9, 0.72: 9, 0.07: 9, 0.61: 9, 0.22: 9, 0.73: 9, 0.79: 9, 0.57: 9, 0.76: 9, 0.53: 9, 0.96: 8, 0.1: 8, 0.74: 8, 0.16: 8, 0.97: 8, 0.24: 8, 0.34: 8, 0.28: 8, 0.87: 8, 0.48: 8, 0.36: 8, 0.68: 8, 0.29: 8, 0.78: 8, 0.47: 8, 0.55: 8, 0.41: 8, 0.26: 8, 0.37: 8, 0.92: 8, 0.19: 7, 0.69: 7, 0.63: 7, 0.91: 7, 0.89: 7, 0.23: 7, 0.03: 6, 0.9: 6, 0.39: 6, 0.05: 6, 0.06: 6, 0.59: 6, 0.08: 6, 0.2: 6, 0.67: 6, 1.0: 6, 0.62: 6, 0.66: 6, 0.94: 6, 0.27: 6, 0.98: 6, 0.15: 6, 0.71: 5, 0.45: 5, 0.35: 5, 0.82: 5, 0.64: 5, 0.52: 5, 0.04: 5, 0.8: 5, 0.49: 5, 0.21: 5, 0.85: 4, 0.75: 4, 0.4: 4, 0.0: 4, 0.33: 4, 0.14: 4, 0.86: 3, 0.43: 3, 0.84: 1}\n",
      "   Column types:\n",
      "     tech_stack: <class 'dict'> - Example: {'python': 0, 'scala': 0, 'spark': 0, 'kafka': 0, 'airflow': 0, 'mongodb': 0, 'flask': 0, 'sql': 0, 'docker': 0, 'tensorflow': 0, 'fastapi': 0}\n",
      "     industry: <class 'str'> - Example: tech\n",
      "     source: <class 'str'> - Example: Recruiter\n",
      "     lang_german_level: <class 'NoneType'> - Example: None\n",
      "     relative_seniority: <class 'numpy.int64'> - Example: 0\n",
      "     processed_text: <class 'str'> - Example: seeking junior software developer years experience office across way follow laugh put agency more white necessary page them approach example community pretty young manage crime force participant eat country skill age different whom throughout try commercial food school determine recently around each court social or final as month only activity call voice whatever cover ahead public become all usually anything election strong know tonight decade tonight\n",
      "     description_lang: <class 'str'> - Example: en\n",
      "ğŸ”§ Creating pipeline...\n",
      "ğŸ“‹ Pipeline steps:\n",
      "   1. columntransformer: ColumnTransformer\n",
      "      Transformers:\n",
      "        - techstacktransformer: TechStackTransformer on ['tech_stack']\n",
      "        - onehotencoder-1: OneHotEncoder on ['industry']\n",
      "        - onehotencoder-2: OneHotEncoder on ['source']\n",
      "        - onehotencoder-3: OneHotEncoder on ['lang_german_level']\n",
      "        - passthrough: str on ['relative_seniority', 'lang_german_required', 'lang_english_required']\n",
      "        - languageawaretfidf: LanguageAwareTfidf on ['processed_text', 'description_lang']\n",
      "   2. gradientboostingregressor: GradientBoostingRegressor\n",
      "ğŸ¯ Fitting pipeline...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0818            3.72s\n",
      "         2           0.0813            2.52s\n",
      "         3           0.0808            2.15s\n",
      "         4           0.0803            1.93s\n",
      "         5           0.0796            1.87s\n",
      "         6           0.0791            1.77s\n",
      "         7           0.0786            1.67s\n",
      "         8           0.0780            1.61s\n",
      "         9           0.0776            1.54s\n",
      "        10           0.0772            1.49s\n",
      "        20           0.0732            1.24s\n",
      "        30           0.0696            1.12s\n",
      "        40           0.0661            1.00s\n",
      "        50           0.0636            0.90s\n",
      "        60           0.0611            0.80s\n",
      "        70           0.0592            0.70s\n",
      "        80           0.0572            0.61s\n",
      "        90           0.0556            0.52s\n",
      "       100           0.0537            0.43s\n",
      "âœ… Training completed in 0:00:02.202708\n",
      "ğŸ‰ Pipeline fitted successfully!\n",
      "ğŸ§ª Quick test prediction: 0.509832315344979\n"
     ]
    }
   ],
   "source": [
    "# Model Training (Complete with debugging)\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ MODEL TRAINING WITH PRODUCTION FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not X_train.empty and not y_train.empty:\n",
    "    print(\"ğŸ“Š Starting training...\")\n",
    "    print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"   Features available: {X_train.shape[1]}\")\n",
    "    print(f\"   Target distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "    \n",
    "    # Debug: Show what columns we have\n",
    "    print(f\"   Column types:\")\n",
    "    for col in ['tech_stack', 'industry', 'source', 'lang_german_level', \n",
    "                'relative_seniority', 'processed_text', 'description_lang']:\n",
    "        if col in X_train.columns:\n",
    "            sample_val = X_train[col].iloc[0] if not X_train.empty else None\n",
    "            print(f\"     {col}: {type(sample_val)} - Example: {sample_val}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Create the production-equivalent pipeline\n",
    "        print(\"ğŸ”§ Creating pipeline...\")\n",
    "        pipeline = create_pipeline()\n",
    "        \n",
    "        # Show pipeline structure\n",
    "        print(\"ğŸ“‹ Pipeline steps:\")\n",
    "        for i, (name, transformer) in enumerate(pipeline.steps):\n",
    "            print(f\"   {i+1}. {name}: {type(transformer).__name__}\")\n",
    "            \n",
    "            # Show column transformer details if it's the first step\n",
    "            if i == 0 and hasattr(transformer, 'transformers'):\n",
    "                print(\"      Transformers:\")\n",
    "                for trans_name, trans, cols in transformer.transformers:\n",
    "                    print(f\"        - {trans_name}: {type(trans).__name__} on {cols}\")\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        print(\"ğŸ¯ Fitting pipeline...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        training_time = datetime.now() - start_time\n",
    "        print(f\"âœ… Training completed in {training_time}\")\n",
    "        \n",
    "        # Check if pipeline was fitted successfully\n",
    "        if hasattr(pipeline, 'fit'):\n",
    "            print(\"ğŸ‰ Pipeline fitted successfully!\")\n",
    "            \n",
    "            # Test a quick prediction to verify it works\n",
    "            try:\n",
    "                sample_pred = pipeline.predict(X_train.head(1))\n",
    "                print(f\"ğŸ§ª Quick test prediction: {sample_pred[0]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Test prediction failed: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed: {e}\")\n",
    "        import traceback\n",
    "        print(\"ğŸ” Stack trace:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Debug: Check specific column issues\n",
    "        print(\"\\nğŸ› Debugging column issues:\")\n",
    "        for col in ['tech_stack', 'processed_text', 'description_lang']:\n",
    "            if col in X_train.columns:\n",
    "                non_null_count = X_train[col].notna().sum()\n",
    "                print(f\"   {col}: {non_null_count} non-null values\")\n",
    "                if X_train[col].notna().any():\n",
    "                    sample = X_train[col].dropna().iloc[0]\n",
    "                    print(f\"     Sample: {sample} (type: {type(sample)})\")\n",
    "        \n",
    "        pipeline = None\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  No training data available\")\n",
    "    print(f\"   X_train empty: {X_train.empty}\")\n",
    "    print(f\"   y_train empty: {y_train.empty if hasattr(y_train, 'empty') else 'N/A'}\")\n",
    "    \n",
    "    if not enhanced_df.empty:\n",
    "        print(f\"   enhanced_df shape: {enhanced_df.shape}\")\n",
    "        print(f\"   enhanced_df columns: {list(enhanced_df.columns)}\")\n",
    "        if 'success_probability' in enhanced_df.columns:\n",
    "            target_not_null = enhanced_df['success_probability'].notna().sum()\n",
    "            print(f\"   Non-null target values: {target_not_null}\")\n",
    "    \n",
    "    pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a633cb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Model Evaluation with Production Features\n",
      "==================================================\n",
      "ğŸ“ˆ Train RÂ² Score: 0.4372\n",
      "ğŸ“ˆ Test RÂ² Score: -0.0711\n",
      "ğŸ“ˆ Mean Squared Error (MSE): 0.0906\n",
      "ğŸ“ˆ Root Mean Squared Error (RMSE): 0.3011\n",
      "ğŸ“ˆ Mean Absolute Error (MAE): 0.2600\n",
      "ğŸ“ˆ RÂ² Score: -0.0711\n",
      "ğŸ“ˆ Explained Variance Score: -0.0501\n",
      "ğŸ“ˆ Max Prediction Error: 0.7216\n",
      "ğŸ“ˆ Mean Prediction: 0.4885\n",
      "ğŸ“ˆ Std of Predictions: 0.0952\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation for Regression\n",
    "if pipeline is not None and not X_test.empty:\n",
    "    print(\"ğŸ“Š Model Evaluation with Production Features\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate regression metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evs = explained_variance_score(y_test, y_pred)\n",
    "        \n",
    "        # Training score\n",
    "        train_score = pipeline.score(X_train, y_train)\n",
    "        test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "        print(f\"ğŸ“ˆ Train RÂ² Score: {train_score:.4f}\")\n",
    "        print(f\"ğŸ“ˆ Test RÂ² Score: {test_score:.4f}\")\n",
    "        print(f\"ğŸ“ˆ Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"ğŸ“ˆ Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"ğŸ“ˆ Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"ğŸ“ˆ RÂ² Score: {r2:.4f}\")\n",
    "        print(f\"ğŸ“ˆ Explained Variance Score: {evs:.4f}\")\n",
    "\n",
    "        # Additional useful metrics for probability prediction\n",
    "        print(f\"ğŸ“ˆ Max Prediction Error: {abs(y_test - y_pred).max():.4f}\")\n",
    "        print(f\"ğŸ“ˆ Mean Prediction: {y_pred.mean():.4f}\")\n",
    "        print(f\"ğŸ“ˆ Std of Predictions: {y_pred.std():.4f}\")\n",
    "\n",
    "        # Scatter plot of predictions vs actual\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.scatter(y_test, y_pred, alpha=0.5, s=30)\n",
    "        # plt.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect Prediction')\n",
    "        # plt.xlabel('Actual Success Probability')\n",
    "        # plt.ylabel('Predicted Success Probability')\n",
    "        # plt.title('Actual vs Predicted Success Probability')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.show()\n",
    "\n",
    "        # Residual plot\n",
    "        # residuals = y_test - y_pred\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.scatter(y_pred, residuals, alpha=0.5, s=30)\n",
    "        # plt.axhline(y=0, color='r', linestyle='--', label='Zero Error')\n",
    "        # plt.xlabel('Predicted Success Probability')\n",
    "        # plt.ylabel('Residuals (Actual - Predicted)')\n",
    "        # plt.title('Residual Plot')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.show()\n",
    "\n",
    "        # Distribution of errors\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "        # plt.xlabel('Prediction Error (Actual - Predicted)')\n",
    "        # plt.ylabel('Frequency')\n",
    "        # plt.title('Distribution of Prediction Errors')\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.show()\n",
    "\n",
    "        # Distribution of actual vs predicted values\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.hist(y_test, bins=30, alpha=0.5, label='Actual', edgecolor='black')\n",
    "        # plt.hist(y_pred, bins=30, alpha=0.5, label='Predicted', edgecolor='black')\n",
    "        # plt.xlabel('Success Probability')\n",
    "        # plt.ylabel('Frequency')\n",
    "        # plt.title('Distribution of Actual vs Predicted Values')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True, alpha=0.3)\n",
    "        # plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Evaluation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot evaluate model - no test data or model not trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1fa847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Model saved to: /home/aleksei/Projects/job_tracker/app/services/job_predictor/models/model_v1.0_20250824_2136.pkl\n",
      "ğŸ’¾ Latest model saved to: /home/aleksei/Projects/job_tracker/app/services/job_predictor/models/enhanced_model.pkl\n",
      "âœ… Saved model verification: ColumnTransformer is fitted\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "def save_model(pipeline: Pipeline, version=MODEL_VERSION) -> None:\n",
    "    \"\"\"Save the trained model with production features.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
    "        \n",
    "        model_filename = (\n",
    "            f\"model_v{version}_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl\"\n",
    "        )\n",
    "        model_path = os.path.join(config.MODEL_DIR, model_filename)\n",
    "\n",
    "        pipeline_to_save = pipeline\n",
    "        \n",
    "        for _, transformer in pipeline_to_save.steps:\n",
    "            if (\n",
    "                hasattr(transformer, 'tech_list_') and \n",
    "                hasattr(transformer.tech_list_, '__iter__')\n",
    "            ):\n",
    "                transformer.tech_list_ = list(transformer.tech_list_)\n",
    "        \n",
    "        joblib.dump(pipeline_to_save, model_path)\n",
    "        \n",
    "        latest_path = os.path.join(config.MODEL_DIR, \"enhanced_model.pkl\")\n",
    "        joblib.dump(pipeline_to_save, latest_path)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Model saved to: {model_path}\")\n",
    "        print(f\"ğŸ’¾ Latest model saved to: {latest_path}\")\n",
    "        \n",
    "        try:\n",
    "            loaded_pipeline = joblib.load(latest_path)\n",
    "            if hasattr(loaded_pipeline.steps[0][1], 'transformers_'):\n",
    "                print(\"âœ… Saved model verification: ColumnTransformer is fitted\")\n",
    "            else:\n",
    "                print(\"âŒ WARNING: Saved model is NOT fitted!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Saved model verification failed: {e}\")\n",
    "        \n",
    "        return model_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Save the model if trained\n",
    "if pipeline is not None:\n",
    "    model_path = save_model(pipeline, version=MODEL_VERSION)\n",
    "else:\n",
    "    print(\"âš ï¸  No model to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edd65c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸  Deleted old model: model_v1.0_20250822_0723.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_v1.0_20250824_2136.pkl',\n",
       " 'model_v1.0_20250822_2321.pkl',\n",
       " 'model_v1.0_20250822_2221.pkl',\n",
       " 'model_v1.0_20250822_2201.pkl',\n",
       " 'model_v1.0_20250822_0732.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a cleanup function\n",
    "def cleanup_old_models(max_versions=5):\n",
    "    \"\"\"Keep only the latest N versions and remove old ones.\"\"\"\n",
    "    model_files = [\n",
    "        f for f in os.listdir(config.MODEL_DIR) \n",
    "        if f.startswith('model_v') and f.endswith('.pkl')\n",
    "    ]\n",
    "    model_files.sort(reverse=True)\n",
    "    \n",
    "    # Keep only the latest N versions\n",
    "    files_to_keep = model_files[:max_versions]\n",
    "    files_to_delete = model_files[max_versions:]\n",
    "    \n",
    "    for file in files_to_delete:\n",
    "        os.remove(os.path.join(config.MODEL_DIR, file))\n",
    "        print(f\"ğŸ—‘ï¸  Deleted old model: {file}\")\n",
    "    \n",
    "    return files_to_keep\n",
    "\n",
    "cleanup_old_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d37273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ§ª PRODUCTION FEATURE PREDICTION EXAMPLE\n",
      "==================================================\n",
      "ğŸ¯ Prediction score: 0.4689\n",
      "ğŸ“Š Interpretation: NOT SUCCESS\n",
      "ğŸ”§ Key features:\n",
      "   - Tech stack: ['python', 'spark', 'sql']\n",
      "   - German level: B2\n",
      "   - German required: True\n",
      "   - Industry: tech\n",
      "   - Relative seniority: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Prediction Example\n",
    "def predict_example(pipeline: Pipeline, example_data: dict[str, str]) -> None:\n",
    "    \"\"\"Make predictions on example data using production features.\"\"\"\n",
    "    try:\n",
    "        example_df = pd.DataFrame([example_data])\n",
    "        enhanced_example = create_enhanced_features(example_df)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = pipeline.predict(enhanced_example)[0]\n",
    "        \n",
    "        # Check if model supports probability predictions\n",
    "        if hasattr(pipeline, 'predict_proba'):\n",
    "            probability = pipeline.predict_proba(enhanced_example)[0][1]\n",
    "            print(f\"ğŸ¯ Prediction: {'SUCCESS' if prediction == 1 else 'NOT SUCCESS'}\")\n",
    "            print(f\"ğŸ“Š Probability: {probability:.4f}\")\n",
    "            print(f\"ğŸ” Confidence: {'HIGH' if probability > 0.7 else 'MEDIUM' if probability > 0.4 else 'LOW'}\")\n",
    "        else:\n",
    "            print(f\"ğŸ¯ Prediction score: {prediction:.4f}\")\n",
    "            print(f\"ğŸ“Š Interpretation: {'SUCCESS' if prediction > 0.5 else 'NOT SUCCESS'}\")\n",
    "            probability = prediction\n",
    "        \n",
    "        # Show feature values for debugging\n",
    "        print(f\"ğŸ”§ Key features:\")\n",
    "        \n",
    "        # Show tech stack keys only (without values)\n",
    "        tech_stack = enhanced_example['tech_stack'].iloc[0]\n",
    "        tech_keys = [tech for tech, present in tech_stack.items() if present == 1]\n",
    "        print(f\"   - Tech stack: {tech_keys}\")\n",
    "        \n",
    "        print(f\"   - German level: {enhanced_example['lang_german_level'].iloc[0]}\")\n",
    "        print(f\"   - German required: {enhanced_example['lang_german_required'].iloc[0]}\")\n",
    "        print(f\"   - Industry: {enhanced_example['industry'].iloc[0]}\")\n",
    "        print(f\"   - Relative seniority: {enhanced_example['relative_seniority'].iloc[0]:.2f}\")\n",
    "        \n",
    "        return prediction, probability\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Prediction failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ§ª PRODUCTION FEATURE PREDICTION EXAMPLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "example_production = {\n",
    "    'company': 'Google Germany',\n",
    "    'location': 'Munich',\n",
    "    'role': 'Senior Data Engineer',\n",
    "    'status': 'Interview/Technical',\n",
    "    'source': 'LinkedIn',\n",
    "    'applied_date': '2024-01-15T10:30:00',\n",
    "    'response_date': '2024-01-22T14:45:00',\n",
    "    'vacancy_description': (\n",
    "        'Senior Data Engineer with 5+ years experience in Python, Spark, '\n",
    "        'and big data technologies. Must have strong SQL skills and '\n",
    "        'experience with AWS. German language skills required (B2 level).'\n",
    "    ),\n",
    "    # Note: tech_stack will be auto-extracted from vacancy_description\n",
    "    # industry will be auto-detected as 'technology'\n",
    "    # language requirements will be auto-extracted\n",
    "}\n",
    "\n",
    "prediction, probability = predict_example(pipeline, example_production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd2c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
